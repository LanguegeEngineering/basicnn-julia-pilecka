# -*- coding: utf-8 -*-
"""numpy_nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L-g9DXmuUpU4WjSqBqMz57oMBviFg5oY

### Zadania

1. Dodać iteracje do treningu
2. Dodać rysunek zmian strat w kolejnych iteracjach
"""

import numpy as np
import matplotlib.pyplot as plt

# N is batch size; D_in is input dimension; liczba przykładów
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 16, 4, 0, 1
#dimensional input - 4 dane mamy u nas; to cechy z danych które wyciągamy tak właściwie #dimensional output - 1 bo chcemy jedno wyjście
# Create random input and output data

x_numpy = np.array(    [[0., 0., 0., 1.],
                        [1., 0., 0., 1.],
                        [0., 1., 0., 1.],
                        [0., 0., 1., 1.],
                        [1., 1., 0., 1.],
                        [1., 0., 1., 1.],
                        [0., 1., 1., 1.],
                        [1., 1., 1., 1.],
                        [0., 0., 0., 0.],
                        [1., 0., 0., 0.],
                        [0., 1., 0., 0.],
                        [0., 0., 1., 0.],
                        [1., 1., 0., 0.],
                        [1., 0., 1., 0.],
                        [0., 1., 1., 0.],
                        [1., 1., 1., 0.]]) #x - dane wejściowe #tu mamy ustawione tak, że ostatnia wartość wyznacza nam labelke, czyli y, reszta to cechy

print(x_numpy.shape) #pokazanie wymiarów, 16 wierszy, 4 kolumny

y_numpy = np.array(     [[1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.]])  #y - labelki/odpowiedzi


print(y_numpy.shape)

# Randomly initialize weights inicjalizujemy wagi  --------------- #pomnożenie 2 macierzy przez siebie tak żeby mieć jeden wynik - macierz 1x1
w = np.random.randn(D_in, D_out)

print(w)
print(w.shape)

learning_rate = 1e-2
loss_list = []

# Forward pass: compute predicted y
y_pred = x_numpy.dot(w) #pierwsza predykacja; po kolejnych predykcjach (poprawki) powinniśmy dostać nasze jedynki i zera jak w y
print(y_pred)

# Compute and print loss
loss = np.square(y_pred - y_numpy).sum() #liczymy strate względem tego co powinno byc w y - ile nasza predykacja roznila sie od wyniku ktory powinien byc; suma tego - wartosc ogolna straty
loss_list.append(loss)
# Backprop to compute gradients of w1 and w2 with respect to loss
grad_y_pred = 2.0 * (y_pred - y_numpy) #liczymy pochodną od funkcji straty. przesuwamy wartości wag w stronę minimum
grad_w = x_numpy.T.dot(grad_y_pred) #gradient dla wag

# Update weights
w = w - learning_rate * grad_w  #zmieniamy wartość wag, żeby dawały mniejszą stratę
print(w)

for i in range (200):
  y_pred = x_numpy.dot(w)
  loss = np.square(y_pred - y_numpy).sum() #liczymy strate względem tego co powinno byc w y - ile nasza predykacja roznila sie od wyniku ktory powinien byc; suma tego - wartosc ogolna straty
  loss_list.append(loss)
# Backprop to compute gradients of w1 and w2 with respect to loss
  grad_y_pred = 2.0 * (y_pred - y_numpy) #liczymy pochodną od funkcji straty. przesuwamy wartości wag w stronę minimum
  grad_w = x_numpy.T.dot(grad_y_pred) #gradient dla wag

# Update weights
  w = w - learning_rate * grad_w  #zmieniamy wartość wag, żeby dawały mniejszą stratę

print(w)
print(y_pred)        #9.99999999e-01 = ok 1

plt.plot(loss_list, label = 'loss')
plt.legend()
plt.show()

